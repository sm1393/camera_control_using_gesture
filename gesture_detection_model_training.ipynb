{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f212a4a5-0edf-4b86-ae4a-ea828bed8d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 19:47:49.088746: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-04 19:47:49.090298: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-04 19:47:49.117481: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-04 19:47:49.118145: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-04 19:47:49.525666: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a593c98-6be0-4ba9-be16-f499ccdd8440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swapnil/.local/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# read the training data set\n",
    "data = pd.read_excel('hand_features_data.xlsx')\n",
    "\n",
    "# top rows of the data\n",
    "data.head()\n",
    "\n",
    "# seperate the independent and target variables\n",
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66e62ff-e854-46b4-9169-301a99643a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>f35</th>\n",
       "      <th>f36</th>\n",
       "      <th>f37</th>\n",
       "      <th>f38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.012502</td>\n",
       "      <td>0.221707</td>\n",
       "      <td>2.438079</td>\n",
       "      <td>0.505950</td>\n",
       "      <td>2.622909</td>\n",
       "      <td>0.757923</td>\n",
       "      <td>3.588601</td>\n",
       "      <td>0.575544</td>\n",
       "      <td>5.053895</td>\n",
       "      <td>0.574802</td>\n",
       "      <td>...</td>\n",
       "      <td>2.638038</td>\n",
       "      <td>0.826151</td>\n",
       "      <td>2.589627</td>\n",
       "      <td>1.173639</td>\n",
       "      <td>3.200658</td>\n",
       "      <td>1.076328</td>\n",
       "      <td>2.741661</td>\n",
       "      <td>1.017778</td>\n",
       "      <td>2.279434</td>\n",
       "      <td>0.980404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.076664</td>\n",
       "      <td>0.194695</td>\n",
       "      <td>2.700717</td>\n",
       "      <td>0.395163</td>\n",
       "      <td>2.917479</td>\n",
       "      <td>0.586849</td>\n",
       "      <td>3.582830</td>\n",
       "      <td>0.615518</td>\n",
       "      <td>5.030980</td>\n",
       "      <td>0.610827</td>\n",
       "      <td>...</td>\n",
       "      <td>2.198519</td>\n",
       "      <td>0.589494</td>\n",
       "      <td>2.634794</td>\n",
       "      <td>1.139123</td>\n",
       "      <td>3.099999</td>\n",
       "      <td>0.945403</td>\n",
       "      <td>2.543667</td>\n",
       "      <td>0.809314</td>\n",
       "      <td>2.043178</td>\n",
       "      <td>0.709426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.959751</td>\n",
       "      <td>0.178725</td>\n",
       "      <td>2.477504</td>\n",
       "      <td>0.366654</td>\n",
       "      <td>2.671019</td>\n",
       "      <td>0.529144</td>\n",
       "      <td>3.136095</td>\n",
       "      <td>0.499009</td>\n",
       "      <td>4.410028</td>\n",
       "      <td>0.493660</td>\n",
       "      <td>...</td>\n",
       "      <td>2.319633</td>\n",
       "      <td>0.517630</td>\n",
       "      <td>2.207489</td>\n",
       "      <td>1.001590</td>\n",
       "      <td>2.730308</td>\n",
       "      <td>0.865236</td>\n",
       "      <td>2.481866</td>\n",
       "      <td>0.742442</td>\n",
       "      <td>2.155402</td>\n",
       "      <td>0.657768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.981047</td>\n",
       "      <td>0.226440</td>\n",
       "      <td>2.600246</td>\n",
       "      <td>0.487529</td>\n",
       "      <td>3.027171</td>\n",
       "      <td>0.655250</td>\n",
       "      <td>3.338329</td>\n",
       "      <td>0.399861</td>\n",
       "      <td>4.717362</td>\n",
       "      <td>0.396842</td>\n",
       "      <td>...</td>\n",
       "      <td>2.526374</td>\n",
       "      <td>0.662258</td>\n",
       "      <td>2.459309</td>\n",
       "      <td>0.931929</td>\n",
       "      <td>3.036416</td>\n",
       "      <td>0.867964</td>\n",
       "      <td>2.635757</td>\n",
       "      <td>0.837925</td>\n",
       "      <td>2.246042</td>\n",
       "      <td>0.839819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.042190</td>\n",
       "      <td>0.251271</td>\n",
       "      <td>2.687520</td>\n",
       "      <td>0.519462</td>\n",
       "      <td>3.181523</td>\n",
       "      <td>0.692673</td>\n",
       "      <td>3.524078</td>\n",
       "      <td>0.433778</td>\n",
       "      <td>4.941984</td>\n",
       "      <td>0.447634</td>\n",
       "      <td>...</td>\n",
       "      <td>2.750091</td>\n",
       "      <td>0.706314</td>\n",
       "      <td>2.669912</td>\n",
       "      <td>0.964991</td>\n",
       "      <td>3.448801</td>\n",
       "      <td>0.907046</td>\n",
       "      <td>3.018056</td>\n",
       "      <td>0.880908</td>\n",
       "      <td>2.598565</td>\n",
       "      <td>0.874303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>2.161567</td>\n",
       "      <td>0.254349</td>\n",
       "      <td>2.955648</td>\n",
       "      <td>0.443055</td>\n",
       "      <td>3.416948</td>\n",
       "      <td>0.556647</td>\n",
       "      <td>3.185656</td>\n",
       "      <td>0.430449</td>\n",
       "      <td>4.506188</td>\n",
       "      <td>0.481145</td>\n",
       "      <td>...</td>\n",
       "      <td>2.444739</td>\n",
       "      <td>0.711023</td>\n",
       "      <td>2.611147</td>\n",
       "      <td>0.932159</td>\n",
       "      <td>3.163002</td>\n",
       "      <td>0.881492</td>\n",
       "      <td>2.745852</td>\n",
       "      <td>0.865313</td>\n",
       "      <td>2.353401</td>\n",
       "      <td>0.871995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>2.417826</td>\n",
       "      <td>0.161811</td>\n",
       "      <td>3.433636</td>\n",
       "      <td>0.307956</td>\n",
       "      <td>4.180878</td>\n",
       "      <td>0.391272</td>\n",
       "      <td>4.153226</td>\n",
       "      <td>0.166086</td>\n",
       "      <td>5.921208</td>\n",
       "      <td>0.206908</td>\n",
       "      <td>...</td>\n",
       "      <td>3.327913</td>\n",
       "      <td>0.488006</td>\n",
       "      <td>3.481176</td>\n",
       "      <td>0.634059</td>\n",
       "      <td>4.209879</td>\n",
       "      <td>0.619682</td>\n",
       "      <td>3.715920</td>\n",
       "      <td>0.631159</td>\n",
       "      <td>3.273808</td>\n",
       "      <td>0.646621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>2.037362</td>\n",
       "      <td>0.174746</td>\n",
       "      <td>2.511528</td>\n",
       "      <td>0.415723</td>\n",
       "      <td>2.625616</td>\n",
       "      <td>0.616940</td>\n",
       "      <td>3.074696</td>\n",
       "      <td>0.413482</td>\n",
       "      <td>4.315129</td>\n",
       "      <td>0.441150</td>\n",
       "      <td>...</td>\n",
       "      <td>2.216096</td>\n",
       "      <td>0.673241</td>\n",
       "      <td>2.406743</td>\n",
       "      <td>0.948844</td>\n",
       "      <td>3.074674</td>\n",
       "      <td>0.895727</td>\n",
       "      <td>2.664676</td>\n",
       "      <td>0.851000</td>\n",
       "      <td>2.259070</td>\n",
       "      <td>0.849139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>2.059372</td>\n",
       "      <td>0.214582</td>\n",
       "      <td>2.579550</td>\n",
       "      <td>0.469940</td>\n",
       "      <td>2.662650</td>\n",
       "      <td>0.661305</td>\n",
       "      <td>3.252017</td>\n",
       "      <td>0.504962</td>\n",
       "      <td>4.553872</td>\n",
       "      <td>0.530775</td>\n",
       "      <td>...</td>\n",
       "      <td>2.445662</td>\n",
       "      <td>0.756775</td>\n",
       "      <td>2.577089</td>\n",
       "      <td>1.030077</td>\n",
       "      <td>3.221709</td>\n",
       "      <td>0.968334</td>\n",
       "      <td>2.841266</td>\n",
       "      <td>0.919292</td>\n",
       "      <td>2.405149</td>\n",
       "      <td>0.916319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1.970517</td>\n",
       "      <td>0.153651</td>\n",
       "      <td>2.449549</td>\n",
       "      <td>0.376519</td>\n",
       "      <td>2.517792</td>\n",
       "      <td>0.581885</td>\n",
       "      <td>2.904929</td>\n",
       "      <td>0.436283</td>\n",
       "      <td>4.119077</td>\n",
       "      <td>0.455108</td>\n",
       "      <td>...</td>\n",
       "      <td>1.936371</td>\n",
       "      <td>0.633043</td>\n",
       "      <td>2.284801</td>\n",
       "      <td>0.967576</td>\n",
       "      <td>2.813351</td>\n",
       "      <td>0.883364</td>\n",
       "      <td>2.440838</td>\n",
       "      <td>0.821834</td>\n",
       "      <td>2.043784</td>\n",
       "      <td>0.807902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f1        f2        f3        f4        f5        f6        f7  \\\n",
       "0    2.012502  0.221707  2.438079  0.505950  2.622909  0.757923  3.588601   \n",
       "1    2.076664  0.194695  2.700717  0.395163  2.917479  0.586849  3.582830   \n",
       "2    1.959751  0.178725  2.477504  0.366654  2.671019  0.529144  3.136095   \n",
       "3    1.981047  0.226440  2.600246  0.487529  3.027171  0.655250  3.338329   \n",
       "4    2.042190  0.251271  2.687520  0.519462  3.181523  0.692673  3.524078   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "395  2.161567  0.254349  2.955648  0.443055  3.416948  0.556647  3.185656   \n",
       "396  2.417826  0.161811  3.433636  0.307956  4.180878  0.391272  4.153226   \n",
       "397  2.037362  0.174746  2.511528  0.415723  2.625616  0.616940  3.074696   \n",
       "398  2.059372  0.214582  2.579550  0.469940  2.662650  0.661305  3.252017   \n",
       "399  1.970517  0.153651  2.449549  0.376519  2.517792  0.581885  2.904929   \n",
       "\n",
       "           f8        f9       f10  ...       f29       f30       f31  \\\n",
       "0    0.575544  5.053895  0.574802  ...  2.638038  0.826151  2.589627   \n",
       "1    0.615518  5.030980  0.610827  ...  2.198519  0.589494  2.634794   \n",
       "2    0.499009  4.410028  0.493660  ...  2.319633  0.517630  2.207489   \n",
       "3    0.399861  4.717362  0.396842  ...  2.526374  0.662258  2.459309   \n",
       "4    0.433778  4.941984  0.447634  ...  2.750091  0.706314  2.669912   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "395  0.430449  4.506188  0.481145  ...  2.444739  0.711023  2.611147   \n",
       "396  0.166086  5.921208  0.206908  ...  3.327913  0.488006  3.481176   \n",
       "397  0.413482  4.315129  0.441150  ...  2.216096  0.673241  2.406743   \n",
       "398  0.504962  4.553872  0.530775  ...  2.445662  0.756775  2.577089   \n",
       "399  0.436283  4.119077  0.455108  ...  1.936371  0.633043  2.284801   \n",
       "\n",
       "          f32       f33       f34       f35       f36       f37       f38  \n",
       "0    1.173639  3.200658  1.076328  2.741661  1.017778  2.279434  0.980404  \n",
       "1    1.139123  3.099999  0.945403  2.543667  0.809314  2.043178  0.709426  \n",
       "2    1.001590  2.730308  0.865236  2.481866  0.742442  2.155402  0.657768  \n",
       "3    0.931929  3.036416  0.867964  2.635757  0.837925  2.246042  0.839819  \n",
       "4    0.964991  3.448801  0.907046  3.018056  0.880908  2.598565  0.874303  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "395  0.932159  3.163002  0.881492  2.745852  0.865313  2.353401  0.871995  \n",
       "396  0.634059  4.209879  0.619682  3.715920  0.631159  3.273808  0.646621  \n",
       "397  0.948844  3.074674  0.895727  2.664676  0.851000  2.259070  0.849139  \n",
       "398  1.030077  3.221709  0.968334  2.841266  0.919292  2.405149  0.916319  \n",
       "399  0.967576  2.813351  0.883364  2.440838  0.821834  2.043784  0.807902  \n",
       "\n",
       "[400 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5d00f1-5ef9-48a4-9725-36727f73dfe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9067834b-9e1d-406d-b796-2c4308eab2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('preprocessing', StandardScaler()),\n",
    "    ('model', MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aece2cc4-e422-4f91-a50e-6b48fd0159b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000,\n",
       "                               random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000,\n",
       "                               random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessing', StandardScaler()),\n",
       "                ('model',\n",
       "                 MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000,\n",
       "                               random_state=42))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50a0dae-0b98-4c89-92ef-d3c42cd78159",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd436e45-20b6-49b2-8480-cb9915c4727d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>f35</th>\n",
       "      <th>f36</th>\n",
       "      <th>f37</th>\n",
       "      <th>f38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.981047</td>\n",
       "      <td>0.226440</td>\n",
       "      <td>2.600246</td>\n",
       "      <td>0.487529</td>\n",
       "      <td>3.027171</td>\n",
       "      <td>0.655250</td>\n",
       "      <td>3.338329</td>\n",
       "      <td>0.399861</td>\n",
       "      <td>4.717362</td>\n",
       "      <td>0.396842</td>\n",
       "      <td>...</td>\n",
       "      <td>2.526374</td>\n",
       "      <td>0.662258</td>\n",
       "      <td>2.459309</td>\n",
       "      <td>0.931929</td>\n",
       "      <td>3.036416</td>\n",
       "      <td>0.867964</td>\n",
       "      <td>2.635757</td>\n",
       "      <td>0.837925</td>\n",
       "      <td>2.246042</td>\n",
       "      <td>0.839819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.980282</td>\n",
       "      <td>0.190757</td>\n",
       "      <td>2.552136</td>\n",
       "      <td>0.448004</td>\n",
       "      <td>2.976979</td>\n",
       "      <td>0.626777</td>\n",
       "      <td>3.336714</td>\n",
       "      <td>0.382817</td>\n",
       "      <td>4.674411</td>\n",
       "      <td>0.367223</td>\n",
       "      <td>...</td>\n",
       "      <td>2.600470</td>\n",
       "      <td>0.637595</td>\n",
       "      <td>2.512289</td>\n",
       "      <td>0.908791</td>\n",
       "      <td>3.203906</td>\n",
       "      <td>0.876646</td>\n",
       "      <td>2.935046</td>\n",
       "      <td>0.856236</td>\n",
       "      <td>2.564439</td>\n",
       "      <td>0.847164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2.258195</td>\n",
       "      <td>0.110442</td>\n",
       "      <td>3.333789</td>\n",
       "      <td>0.153176</td>\n",
       "      <td>4.101253</td>\n",
       "      <td>0.239631</td>\n",
       "      <td>3.135008</td>\n",
       "      <td>0.728342</td>\n",
       "      <td>4.404382</td>\n",
       "      <td>0.651289</td>\n",
       "      <td>...</td>\n",
       "      <td>4.592178</td>\n",
       "      <td>0.479192</td>\n",
       "      <td>3.259128</td>\n",
       "      <td>0.863947</td>\n",
       "      <td>4.406711</td>\n",
       "      <td>0.772724</td>\n",
       "      <td>4.709074</td>\n",
       "      <td>0.651549</td>\n",
       "      <td>4.644441</td>\n",
       "      <td>0.547724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2.060245</td>\n",
       "      <td>-0.089847</td>\n",
       "      <td>2.914222</td>\n",
       "      <td>-0.280769</td>\n",
       "      <td>3.500561</td>\n",
       "      <td>-0.421476</td>\n",
       "      <td>3.110428</td>\n",
       "      <td>0.216461</td>\n",
       "      <td>4.188876</td>\n",
       "      <td>-0.114094</td>\n",
       "      <td>...</td>\n",
       "      <td>4.015106</td>\n",
       "      <td>-0.531220</td>\n",
       "      <td>2.917031</td>\n",
       "      <td>-0.220481</td>\n",
       "      <td>4.053991</td>\n",
       "      <td>-0.349768</td>\n",
       "      <td>4.235718</td>\n",
       "      <td>-0.470354</td>\n",
       "      <td>4.142197</td>\n",
       "      <td>-0.575816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1.863256</td>\n",
       "      <td>0.122767</td>\n",
       "      <td>2.498671</td>\n",
       "      <td>0.137815</td>\n",
       "      <td>2.999532</td>\n",
       "      <td>0.208602</td>\n",
       "      <td>2.536791</td>\n",
       "      <td>0.715022</td>\n",
       "      <td>3.411440</td>\n",
       "      <td>0.634085</td>\n",
       "      <td>...</td>\n",
       "      <td>3.063640</td>\n",
       "      <td>0.367633</td>\n",
       "      <td>2.322591</td>\n",
       "      <td>0.821027</td>\n",
       "      <td>3.203450</td>\n",
       "      <td>0.662251</td>\n",
       "      <td>3.299719</td>\n",
       "      <td>0.508629</td>\n",
       "      <td>3.092582</td>\n",
       "      <td>0.384939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.963492</td>\n",
       "      <td>0.230614</td>\n",
       "      <td>2.193964</td>\n",
       "      <td>0.584262</td>\n",
       "      <td>2.223537</td>\n",
       "      <td>0.918228</td>\n",
       "      <td>3.720520</td>\n",
       "      <td>0.588667</td>\n",
       "      <td>5.259868</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>...</td>\n",
       "      <td>2.481968</td>\n",
       "      <td>0.939194</td>\n",
       "      <td>2.617972</td>\n",
       "      <td>1.222433</td>\n",
       "      <td>2.926729</td>\n",
       "      <td>1.205619</td>\n",
       "      <td>2.578422</td>\n",
       "      <td>1.142173</td>\n",
       "      <td>2.247784</td>\n",
       "      <td>1.065854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2.205534</td>\n",
       "      <td>0.212420</td>\n",
       "      <td>3.083237</td>\n",
       "      <td>0.226912</td>\n",
       "      <td>3.706692</td>\n",
       "      <td>0.178636</td>\n",
       "      <td>3.234621</td>\n",
       "      <td>0.846258</td>\n",
       "      <td>4.647375</td>\n",
       "      <td>0.960303</td>\n",
       "      <td>...</td>\n",
       "      <td>1.640913</td>\n",
       "      <td>1.055066</td>\n",
       "      <td>2.795589</td>\n",
       "      <td>1.436533</td>\n",
       "      <td>3.293329</td>\n",
       "      <td>1.339199</td>\n",
       "      <td>2.602399</td>\n",
       "      <td>1.300583</td>\n",
       "      <td>2.116554</td>\n",
       "      <td>1.295126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2.034480</td>\n",
       "      <td>0.134898</td>\n",
       "      <td>2.802252</td>\n",
       "      <td>0.159618</td>\n",
       "      <td>3.332019</td>\n",
       "      <td>0.223839</td>\n",
       "      <td>2.637724</td>\n",
       "      <td>0.674473</td>\n",
       "      <td>3.621190</td>\n",
       "      <td>0.606618</td>\n",
       "      <td>...</td>\n",
       "      <td>3.325915</td>\n",
       "      <td>0.328519</td>\n",
       "      <td>2.372813</td>\n",
       "      <td>0.830130</td>\n",
       "      <td>3.276153</td>\n",
       "      <td>0.650877</td>\n",
       "      <td>3.398877</td>\n",
       "      <td>0.478530</td>\n",
       "      <td>3.229788</td>\n",
       "      <td>0.341271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>2.082663</td>\n",
       "      <td>0.183622</td>\n",
       "      <td>2.707885</td>\n",
       "      <td>0.294231</td>\n",
       "      <td>3.094066</td>\n",
       "      <td>0.336056</td>\n",
       "      <td>3.498492</td>\n",
       "      <td>0.635915</td>\n",
       "      <td>5.025318</td>\n",
       "      <td>0.661840</td>\n",
       "      <td>...</td>\n",
       "      <td>2.273898</td>\n",
       "      <td>0.547746</td>\n",
       "      <td>2.668973</td>\n",
       "      <td>1.077539</td>\n",
       "      <td>3.170820</td>\n",
       "      <td>0.870686</td>\n",
       "      <td>2.685613</td>\n",
       "      <td>0.733142</td>\n",
       "      <td>2.224195</td>\n",
       "      <td>0.680769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2.288603</td>\n",
       "      <td>0.276457</td>\n",
       "      <td>3.372100</td>\n",
       "      <td>0.339266</td>\n",
       "      <td>4.181429</td>\n",
       "      <td>0.302883</td>\n",
       "      <td>3.393572</td>\n",
       "      <td>0.910574</td>\n",
       "      <td>4.999908</td>\n",
       "      <td>1.036373</td>\n",
       "      <td>...</td>\n",
       "      <td>1.825982</td>\n",
       "      <td>1.268141</td>\n",
       "      <td>2.916091</td>\n",
       "      <td>1.514730</td>\n",
       "      <td>3.418064</td>\n",
       "      <td>1.520870</td>\n",
       "      <td>2.542858</td>\n",
       "      <td>1.538003</td>\n",
       "      <td>2.025147</td>\n",
       "      <td>1.515231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f1        f2        f3        f4        f5        f6        f7  \\\n",
       "3    1.981047  0.226440  2.600246  0.487529  3.027171  0.655250  3.338329   \n",
       "18   1.980282  0.190757  2.552136  0.448004  2.976979  0.626777  3.336714   \n",
       "202  2.258195  0.110442  3.333789  0.153176  4.101253  0.239631  3.135008   \n",
       "250  2.060245 -0.089847  2.914222 -0.280769  3.500561 -0.421476  3.110428   \n",
       "274  1.863256  0.122767  2.498671  0.137815  2.999532  0.208602  2.536791   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "71   1.963492  0.230614  2.193964  0.584262  2.223537  0.918228  3.720520   \n",
       "106  2.205534  0.212420  3.083237  0.226912  3.706692  0.178636  3.234621   \n",
       "270  2.034480  0.134898  2.802252  0.159618  3.332019  0.223839  2.637724   \n",
       "348  2.082663  0.183622  2.707885  0.294231  3.094066  0.336056  3.498492   \n",
       "102  2.288603  0.276457  3.372100  0.339266  4.181429  0.302883  3.393572   \n",
       "\n",
       "           f8        f9       f10  ...       f29       f30       f31  \\\n",
       "3    0.399861  4.717362  0.396842  ...  2.526374  0.662258  2.459309   \n",
       "18   0.382817  4.674411  0.367223  ...  2.600470  0.637595  2.512289   \n",
       "202  0.728342  4.404382  0.651289  ...  4.592178  0.479192  3.259128   \n",
       "250  0.216461  4.188876 -0.114094  ...  4.015106 -0.531220  2.917031   \n",
       "274  0.715022  3.411440  0.634085  ...  3.063640  0.367633  2.322591   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "71   0.588667  5.259868  0.610200  ...  2.481968  0.939194  2.617972   \n",
       "106  0.846258  4.647375  0.960303  ...  1.640913  1.055066  2.795589   \n",
       "270  0.674473  3.621190  0.606618  ...  3.325915  0.328519  2.372813   \n",
       "348  0.635915  5.025318  0.661840  ...  2.273898  0.547746  2.668973   \n",
       "102  0.910574  4.999908  1.036373  ...  1.825982  1.268141  2.916091   \n",
       "\n",
       "          f32       f33       f34       f35       f36       f37       f38  \n",
       "3    0.931929  3.036416  0.867964  2.635757  0.837925  2.246042  0.839819  \n",
       "18   0.908791  3.203906  0.876646  2.935046  0.856236  2.564439  0.847164  \n",
       "202  0.863947  4.406711  0.772724  4.709074  0.651549  4.644441  0.547724  \n",
       "250 -0.220481  4.053991 -0.349768  4.235718 -0.470354  4.142197 -0.575816  \n",
       "274  0.821027  3.203450  0.662251  3.299719  0.508629  3.092582  0.384939  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "71   1.222433  2.926729  1.205619  2.578422  1.142173  2.247784  1.065854  \n",
       "106  1.436533  3.293329  1.339199  2.602399  1.300583  2.116554  1.295126  \n",
       "270  0.830130  3.276153  0.650877  3.398877  0.478530  3.229788  0.341271  \n",
       "348  1.077539  3.170820  0.870686  2.685613  0.733142  2.224195  0.680769  \n",
       "102  1.514730  3.418064  1.520870  2.542858  1.538003  2.025147  1.515231  \n",
       "\n",
       "[320 rows x 38 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ddd94ca-8233-4614-8fc6-9cc972ecec1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape =  (320, 38)\n",
      "X_test.shape =  (80, 38)\n",
      "y_train.shape =  (320, 4)\n",
      "y_test.shape =  (80, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape = \", X_train.shape)\n",
    "print(\"X_test.shape = \", X_test.shape)\n",
    "print(\"y_train.shape = \", y_train.shape)\n",
    "print(\"y_test.shape = \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4790a1f0-03c3-4fb7-a41b-952a0153fa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000,\n",
       "                               random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000,\n",
       "                               random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessing', StandardScaler()),\n",
       "                ('model',\n",
       "                 MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000,\n",
       "                               random_state=42))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline.named_steps['model'].compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# pipeline.fit(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f322106b-6ffd-4771-ba80-d883df9cbac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2279b41-1378-4f30-9bd1-1345f1d6fdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>f35</th>\n",
       "      <th>f36</th>\n",
       "      <th>f37</th>\n",
       "      <th>f38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1.847600</td>\n",
       "      <td>0.083193</td>\n",
       "      <td>2.583246</td>\n",
       "      <td>0.106759</td>\n",
       "      <td>3.209357</td>\n",
       "      <td>0.186910</td>\n",
       "      <td>2.567246</td>\n",
       "      <td>0.693712</td>\n",
       "      <td>3.458652</td>\n",
       "      <td>0.573685</td>\n",
       "      <td>...</td>\n",
       "      <td>3.378430</td>\n",
       "      <td>0.367142</td>\n",
       "      <td>2.655380</td>\n",
       "      <td>0.840458</td>\n",
       "      <td>3.595159</td>\n",
       "      <td>0.695751</td>\n",
       "      <td>3.692031</td>\n",
       "      <td>0.544641</td>\n",
       "      <td>3.462018</td>\n",
       "      <td>0.417705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1.950270</td>\n",
       "      <td>0.108919</td>\n",
       "      <td>2.692431</td>\n",
       "      <td>0.107159</td>\n",
       "      <td>3.295694</td>\n",
       "      <td>0.127215</td>\n",
       "      <td>2.627181</td>\n",
       "      <td>0.747545</td>\n",
       "      <td>3.590205</td>\n",
       "      <td>0.550202</td>\n",
       "      <td>...</td>\n",
       "      <td>3.590748</td>\n",
       "      <td>0.251681</td>\n",
       "      <td>2.339853</td>\n",
       "      <td>0.772075</td>\n",
       "      <td>3.313077</td>\n",
       "      <td>0.568669</td>\n",
       "      <td>3.562154</td>\n",
       "      <td>0.419301</td>\n",
       "      <td>3.473159</td>\n",
       "      <td>0.302920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.954374</td>\n",
       "      <td>0.182970</td>\n",
       "      <td>2.453901</td>\n",
       "      <td>0.412537</td>\n",
       "      <td>2.664701</td>\n",
       "      <td>0.610366</td>\n",
       "      <td>3.277917</td>\n",
       "      <td>0.569226</td>\n",
       "      <td>4.618153</td>\n",
       "      <td>0.574773</td>\n",
       "      <td>...</td>\n",
       "      <td>2.190958</td>\n",
       "      <td>0.662255</td>\n",
       "      <td>2.334265</td>\n",
       "      <td>1.122151</td>\n",
       "      <td>2.834243</td>\n",
       "      <td>0.987395</td>\n",
       "      <td>2.489584</td>\n",
       "      <td>0.876940</td>\n",
       "      <td>2.108695</td>\n",
       "      <td>0.811733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1.798824</td>\n",
       "      <td>0.102272</td>\n",
       "      <td>2.430282</td>\n",
       "      <td>0.115344</td>\n",
       "      <td>2.969394</td>\n",
       "      <td>0.181245</td>\n",
       "      <td>2.678129</td>\n",
       "      <td>0.789746</td>\n",
       "      <td>3.457939</td>\n",
       "      <td>0.585569</td>\n",
       "      <td>...</td>\n",
       "      <td>3.190480</td>\n",
       "      <td>0.279548</td>\n",
       "      <td>2.379424</td>\n",
       "      <td>0.738941</td>\n",
       "      <td>3.184874</td>\n",
       "      <td>0.559543</td>\n",
       "      <td>3.286011</td>\n",
       "      <td>0.417826</td>\n",
       "      <td>3.159874</td>\n",
       "      <td>0.305091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.966653</td>\n",
       "      <td>0.226387</td>\n",
       "      <td>2.607252</td>\n",
       "      <td>0.448858</td>\n",
       "      <td>3.081276</td>\n",
       "      <td>0.554734</td>\n",
       "      <td>3.062288</td>\n",
       "      <td>0.389782</td>\n",
       "      <td>4.386879</td>\n",
       "      <td>0.378866</td>\n",
       "      <td>...</td>\n",
       "      <td>2.296641</td>\n",
       "      <td>0.553082</td>\n",
       "      <td>2.325881</td>\n",
       "      <td>0.926845</td>\n",
       "      <td>2.847837</td>\n",
       "      <td>0.873869</td>\n",
       "      <td>2.506073</td>\n",
       "      <td>0.779640</td>\n",
       "      <td>2.184210</td>\n",
       "      <td>0.702695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1.862566</td>\n",
       "      <td>0.061685</td>\n",
       "      <td>2.488923</td>\n",
       "      <td>0.061695</td>\n",
       "      <td>2.909706</td>\n",
       "      <td>0.141017</td>\n",
       "      <td>2.368437</td>\n",
       "      <td>0.555053</td>\n",
       "      <td>3.192141</td>\n",
       "      <td>0.498989</td>\n",
       "      <td>...</td>\n",
       "      <td>2.946165</td>\n",
       "      <td>0.231623</td>\n",
       "      <td>2.182050</td>\n",
       "      <td>0.738259</td>\n",
       "      <td>2.915334</td>\n",
       "      <td>0.584472</td>\n",
       "      <td>2.998719</td>\n",
       "      <td>0.407035</td>\n",
       "      <td>2.854446</td>\n",
       "      <td>0.264536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>1.913729</td>\n",
       "      <td>0.114355</td>\n",
       "      <td>2.647171</td>\n",
       "      <td>0.099249</td>\n",
       "      <td>3.263440</td>\n",
       "      <td>0.113351</td>\n",
       "      <td>2.771297</td>\n",
       "      <td>0.781695</td>\n",
       "      <td>3.560705</td>\n",
       "      <td>0.536432</td>\n",
       "      <td>...</td>\n",
       "      <td>3.804768</td>\n",
       "      <td>0.262186</td>\n",
       "      <td>2.764754</td>\n",
       "      <td>0.700982</td>\n",
       "      <td>3.712678</td>\n",
       "      <td>0.550588</td>\n",
       "      <td>3.861945</td>\n",
       "      <td>0.412043</td>\n",
       "      <td>3.769470</td>\n",
       "      <td>0.292133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2.132688</td>\n",
       "      <td>0.264967</td>\n",
       "      <td>2.670192</td>\n",
       "      <td>0.548165</td>\n",
       "      <td>2.952889</td>\n",
       "      <td>0.747995</td>\n",
       "      <td>3.389234</td>\n",
       "      <td>0.562588</td>\n",
       "      <td>4.777285</td>\n",
       "      <td>0.592961</td>\n",
       "      <td>...</td>\n",
       "      <td>2.167548</td>\n",
       "      <td>0.748919</td>\n",
       "      <td>2.472999</td>\n",
       "      <td>1.157080</td>\n",
       "      <td>3.000566</td>\n",
       "      <td>1.046132</td>\n",
       "      <td>2.397874</td>\n",
       "      <td>0.974853</td>\n",
       "      <td>1.969830</td>\n",
       "      <td>0.967749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2.333770</td>\n",
       "      <td>0.221057</td>\n",
       "      <td>3.303869</td>\n",
       "      <td>0.237875</td>\n",
       "      <td>4.011177</td>\n",
       "      <td>0.206768</td>\n",
       "      <td>3.315697</td>\n",
       "      <td>0.776018</td>\n",
       "      <td>4.719795</td>\n",
       "      <td>0.877890</td>\n",
       "      <td>...</td>\n",
       "      <td>2.056955</td>\n",
       "      <td>0.977995</td>\n",
       "      <td>2.666910</td>\n",
       "      <td>1.305673</td>\n",
       "      <td>3.002715</td>\n",
       "      <td>1.268401</td>\n",
       "      <td>2.350581</td>\n",
       "      <td>1.239222</td>\n",
       "      <td>2.077483</td>\n",
       "      <td>1.202052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>2.041956</td>\n",
       "      <td>0.099248</td>\n",
       "      <td>2.903459</td>\n",
       "      <td>0.131090</td>\n",
       "      <td>3.477383</td>\n",
       "      <td>0.240712</td>\n",
       "      <td>2.720722</td>\n",
       "      <td>0.654671</td>\n",
       "      <td>3.713421</td>\n",
       "      <td>0.627011</td>\n",
       "      <td>...</td>\n",
       "      <td>3.477480</td>\n",
       "      <td>0.413947</td>\n",
       "      <td>2.673722</td>\n",
       "      <td>0.842132</td>\n",
       "      <td>3.633668</td>\n",
       "      <td>0.712819</td>\n",
       "      <td>3.758397</td>\n",
       "      <td>0.560446</td>\n",
       "      <td>3.530388</td>\n",
       "      <td>0.445562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f1        f2        f3        f4        f5        f6        f7  \\\n",
       "209  1.847600  0.083193  2.583246  0.106759  3.209357  0.186910  2.567246   \n",
       "280  1.950270  0.108919  2.692431  0.107159  3.295694  0.127215  2.627181   \n",
       "33   1.954374  0.182970  2.453901  0.412537  2.664701  0.610366  3.277917   \n",
       "210  1.798824  0.102272  2.430282  0.115344  2.969394  0.181245  2.678129   \n",
       "93   1.966653  0.226387  2.607252  0.448858  3.081276  0.554734  3.062288   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "246  1.862566  0.061685  2.488923  0.061695  2.909706  0.141017  2.368437   \n",
       "227  1.913729  0.114355  2.647171  0.099249  3.263440  0.113351  2.771297   \n",
       "369  2.132688  0.264967  2.670192  0.548165  2.952889  0.747995  3.389234   \n",
       "176  2.333770  0.221057  3.303869  0.237875  4.011177  0.206768  3.315697   \n",
       "289  2.041956  0.099248  2.903459  0.131090  3.477383  0.240712  2.720722   \n",
       "\n",
       "           f8        f9       f10  ...       f29       f30       f31  \\\n",
       "209  0.693712  3.458652  0.573685  ...  3.378430  0.367142  2.655380   \n",
       "280  0.747545  3.590205  0.550202  ...  3.590748  0.251681  2.339853   \n",
       "33   0.569226  4.618153  0.574773  ...  2.190958  0.662255  2.334265   \n",
       "210  0.789746  3.457939  0.585569  ...  3.190480  0.279548  2.379424   \n",
       "93   0.389782  4.386879  0.378866  ...  2.296641  0.553082  2.325881   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "246  0.555053  3.192141  0.498989  ...  2.946165  0.231623  2.182050   \n",
       "227  0.781695  3.560705  0.536432  ...  3.804768  0.262186  2.764754   \n",
       "369  0.562588  4.777285  0.592961  ...  2.167548  0.748919  2.472999   \n",
       "176  0.776018  4.719795  0.877890  ...  2.056955  0.977995  2.666910   \n",
       "289  0.654671  3.713421  0.627011  ...  3.477480  0.413947  2.673722   \n",
       "\n",
       "          f32       f33       f34       f35       f36       f37       f38  \n",
       "209  0.840458  3.595159  0.695751  3.692031  0.544641  3.462018  0.417705  \n",
       "280  0.772075  3.313077  0.568669  3.562154  0.419301  3.473159  0.302920  \n",
       "33   1.122151  2.834243  0.987395  2.489584  0.876940  2.108695  0.811733  \n",
       "210  0.738941  3.184874  0.559543  3.286011  0.417826  3.159874  0.305091  \n",
       "93   0.926845  2.847837  0.873869  2.506073  0.779640  2.184210  0.702695  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "246  0.738259  2.915334  0.584472  2.998719  0.407035  2.854446  0.264536  \n",
       "227  0.700982  3.712678  0.550588  3.861945  0.412043  3.769470  0.292133  \n",
       "369  1.157080  3.000566  1.046132  2.397874  0.974853  1.969830  0.967749  \n",
       "176  1.305673  3.002715  1.268401  2.350581  1.239222  2.077483  1.202052  \n",
       "289  0.842132  3.633668  0.712819  3.758397  0.560446  3.530388  0.445562  \n",
       "\n",
       "[80 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f78bc47a-bf54-4b8c-b474-3c4d554af9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]]\n",
      "0  ?  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swapnil/.local/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/swapnil/.local/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "instance = 31\n",
    "test_instance = X_test.iloc[instance].values\n",
    "print(pipeline.predict(test_instance.reshape(1, -1)))\n",
    "test_instance_prediction = np.argmax(pipeline.predict(test_instance.reshape(1, -1)), axis=1)\n",
    "print(np.argmax(y_test[instance]), ' ? ', test_instance_prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec9339a9-6224-42da-bb0f-d92bb00d100f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.37189476, 0.20876813, 3.27931701, 0.22809622, 3.99007421,\n",
       "       0.20324435, 3.49472386, 0.72169275, 4.93924163, 0.76643525,\n",
       "       5.82767092, 0.79314641, 6.66723016, 0.81611102, 3.44074815,\n",
       "       0.90026525, 3.5047695 , 0.80306443, 2.71771593, 0.74353703,\n",
       "       2.47169171, 0.77060461, 3.20650712, 1.08996897, 3.09429452,\n",
       "       0.98058483, 2.38436277, 0.9434237 , 2.28095253, 0.9771524 ,\n",
       "       2.97626788, 1.32041967, 2.99594091, 1.2027779 , 2.465122  ,\n",
       "       1.15910754, 2.30601824, 1.16389145])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b94a0c6a-d1a2-4158-a8be-43d23debe9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.37189476, 0.20876813, 3.27931701, 0.22809622, 3.99007421,\n",
       "        0.20324435, 3.49472386, 0.72169275, 4.93924163, 0.76643525,\n",
       "        5.82767092, 0.79314641, 6.66723016, 0.81611102, 3.44074815,\n",
       "        0.90026525, 3.5047695 , 0.80306443, 2.71771593, 0.74353703,\n",
       "        2.47169171, 0.77060461, 3.20650712, 1.08996897, 3.09429452,\n",
       "        0.98058483, 2.38436277, 0.9434237 , 2.28095253, 0.9771524 ,\n",
       "        2.97626788, 1.32041967, 2.99594091, 1.2027779 , 2.465122  ,\n",
       "        1.15910754, 2.30601824, 1.16389145]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_instance.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e840602-fa48-44b4-9eae-511638067a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_pipeline.joblib']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(pipeline, 'my_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e8ab09-5256-4860-8a37-fe55d85da94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_pipeline = joblib.load('my_pipeline.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "178b30cc-0019-4094-a3b0-f142cbd3b0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000,\n",
       "                               random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000,\n",
       "                               random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessing', StandardScaler()),\n",
       "                ('model',\n",
       "                 MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000,\n",
       "                               random_state=42))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6108f60-4d56-4ded-aa94-e1e888347627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  ?  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swapnil/.local/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "instance = 30\n",
    "test_instance = X_test.iloc[instance].values\n",
    "test_instance_prediction = np.argmax(loaded_pipeline.predict(test_instance.reshape(1, -1)), axis=1)\n",
    "print(np.argmax(y_test[instance]), ' ? ', test_instance_prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3224b-07df-4cc5-8bbe-821f50b2c3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
